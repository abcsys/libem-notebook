{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyze provided results from paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "dataset_names = {\n",
    "    # 'abt-buy-sampled-gs_domain-complex-force': 'abt-buy-gs',\n",
    "    # 'walmart-amazon-sampled-gs_domain-complex-force': 'walmart-amazon-gs',\n",
    "    # 'amazon-google-sampled-gs_domain-complex-force': 'amazon-google-gs',\n",
    "    'dblp-scholar-sampled-gs_domain-complex-force': 'dblp-scholar-gs',\n",
    "    # 'wdcproducts-80cc-seen-sampled-250-gs-2_domain-complex-force': 'wdcproducts-80cc-seen-gs',\n",
    "    # 'dblp-acm-sampled-gs_domain-complex-force': 'dblp-acm-gs'\n",
    "}\n",
    "\n",
    "def format_output_prod(line):\n",
    "    split = line.split('\\n')\n",
    "    assert(len(split) == 2)\n",
    "    return {\n",
    "        \"Product 1\": split[0][12:-1],\n",
    "        \"Product 2\": split[1][12:-1]\n",
    "    }\n",
    "\n",
    "def format_output_pub(line):\n",
    "    split = line.split('\\n')\n",
    "    assert(len(split) == 2)\n",
    "    return {\n",
    "        \"Publication 1\": split[0][16:-1],\n",
    "        \"Publication 2\": split[1][16:-1]\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_pos_all, true_neg_all, false_pos_all, false_neg_all = {}, {}, {}, {}\n",
    "accuracy, f1, precision, recall = {}, {}, {}, {}\n",
    "\n",
    "for d in dataset_names.keys():\n",
    "\n",
    "    # read in data\n",
    "    if d != 'dblp-acm-sampled-gs_domain-complex-force':\n",
    "        with open(f'./LLMForEM/tasks/{d}.json', 'r') as f:\n",
    "            dataset = json.load(f)['examples']\n",
    "    else:\n",
    "        with open(f'./datasets/{d}.json', 'r') as f:\n",
    "            dataset = [json.loads(line) for line in f]\n",
    "    with open(f'./LLMForEM/prompt-answer-combined/prompts_and_answers/{d}_default_gpt-4-0613_run-1.jsonl', 'r') as f:\n",
    "        results = [json.loads(line) for line in f]\n",
    "\n",
    "    # process results\n",
    "    true_pos, true_neg, false_pos, false_neg, bad_format = [], [], [], [], []\n",
    "    correct = 0\n",
    "\n",
    "    for i in range(len(results)):\n",
    "        # make sure prompt matches and format output\n",
    "        if (d == 'dblp-scholar-sampled-gs_domain-complex-force'):\n",
    "            assert(results[i]['prompt'][120:] == dataset[i]['input'])\n",
    "            data = format_output_pub(dataset[i]['input'])\n",
    "        else:\n",
    "            assert(results[i]['prompt'][124:] == dataset[i]['input'])\n",
    "            data = format_output_prod(dataset[i]['input'])\n",
    "        \n",
    "        if (results[i]['answer'] == 'Yes'):\n",
    "            if (dataset[i]['target_scores']['Yes'] == 1):\n",
    "                true_pos.append(data)\n",
    "                correct += 1\n",
    "            else:\n",
    "                false_pos.append(data)\n",
    "        elif (results[i]['answer'] == 'No'):\n",
    "            if (dataset[i]['target_scores']['No'] == 1):\n",
    "                true_neg.append(data)\n",
    "                correct += 1\n",
    "            else:\n",
    "                false_neg.append(data)\n",
    "        else:\n",
    "            bad_format.append(results[i])\n",
    "    \n",
    "    assert(len(bad_format) == 0)\n",
    "    # print(len(results))\n",
    "    name = dataset_names[d]\n",
    "    \n",
    "    true_pos_all[name], true_neg_all[name], false_pos_all[name], false_neg_all[name] = true_pos, true_neg, false_pos, false_neg\n",
    "    accuracy[name], precision[name], recall[name] = correct/len(data), len(true_pos) / (len(true_pos) + len(false_pos)), len(true_pos) / (len(true_pos) + len(false_neg))\n",
    "    f1[name] = 2 * len(true_pos) / (2 * len(true_pos) + len(false_pos) + len(false_neg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f'./paper-results/true_pos.json', 'w') as f:\n",
    "        json.dump(true_pos_all, f, indent=4)\n",
    "with open(f'./paper-results/true_neg.json', 'w') as f:\n",
    "        json.dump(true_neg_all, f, indent=4)\n",
    "with open(f'./paper-results/false_pos.json', 'w') as f:\n",
    "        json.dump(false_pos_all, f, indent=4)\n",
    "with open(f'./paper-results/false_neg.json', 'w') as f:\n",
    "        json.dump(false_neg_all, f, indent=4)\n",
    "with open(f'./paper-results/f1.json', 'w') as f:\n",
    "        json.dump(f1, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame()\n",
    "df['Precision'] = pd.Series(precision)\n",
    "df['Recall'] = pd.Series(recall)\n",
    "df['F1'] = pd.Series(f1)\n",
    "df['Precision'] *= 100\n",
    "df['Recall'] *= 100\n",
    "df['F1'] *= 100\n",
    "df.round(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyze reproduced paper (no schema) results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "folder = 'no-schema-gpt4-turbo'\n",
    "\n",
    "dataset_names = {\n",
    "    'abt-buy-sampled-gs_domain-complex-force': 'abt-buy-gs',\n",
    "    'walmart-amazon-sampled-gs_domain-complex-force': 'walmart-amazon-gs',\n",
    "    'amazon-google-sampled-gs_domain-complex-force': 'amazon-google-gs',\n",
    "    'dblp-scholar-sampled-gs_domain-complex-force': 'dblp-scholar-gs',\n",
    "    # 'wdcproducts-80cc-seen-sampled-250-gs-2_domain-complex-force': 'wdcproducts-80cc-seen-gs',\n",
    "    'dblp-acm-sampled-gs_domain-complex-force': 'dblp-acm-gs'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_output_prod(line):\n",
    "    split = line.split('\\n')\n",
    "    assert(len(split) == 2)\n",
    "    return {\n",
    "        \"Product 1\": split[0][12:-1],\n",
    "        \"Product 2\": split[1][12:-1]\n",
    "    }\n",
    "\n",
    "def format_output_pub(line):\n",
    "    split = line.split('\\n')\n",
    "    assert(len(split) == 2)\n",
    "    return {\n",
    "        \"Publication 1\": split[0][16:-1],\n",
    "        \"Publication 2\": split[1][16:-1]\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_pos_all, true_neg_all, false_pos_all, false_neg_all = {}, {}, {}, {}\n",
    "accuracy, f1, precision, recall = {}, {}, {}, {}\n",
    "\n",
    "for d in dataset_names.keys():\n",
    "    \n",
    "    # read in results\n",
    "    if d != 'dblp-acm-sampled-gs_domain-complex-force':\n",
    "        with open(f'./LLMForEM/tasks/{d}.json', 'r') as f:\n",
    "            dataset = json.load(f)['examples']\n",
    "    with open(f\"./{folder}/{d}_results.json\", \"r\") as f:\n",
    "        results = json.load(f)\n",
    "        \n",
    "    # process results\n",
    "    true_pos, true_neg, false_pos, false_neg = [], [], [], []\n",
    "    correct, pos, neg = 0, 0, 0\n",
    "\n",
    "    for i in range(len(results)):\n",
    "        if d != 'dblp-acm-sampled-gs_domain-complex-force':\n",
    "            # make sure prompt matches and format output\n",
    "            if (d == 'dblp-scholar-sampled-gs_domain-complex-force'):\n",
    "                assert(results[i]['prompt'][120:] == dataset[i]['input'])\n",
    "                data = format_output_pub(dataset[i]['input'])\n",
    "            else:\n",
    "                assert(results[i]['prompt'][124:] == dataset[i]['input'])\n",
    "                data = format_output_prod(dataset[i]['input'])\n",
    "            \n",
    "            if (results[i]['answer'][0:3] == 'Yes'):\n",
    "                if (dataset[i]['target_scores']['Yes'] == 1):\n",
    "                    true_pos.append(data)\n",
    "                    correct += 1\n",
    "                else:\n",
    "                    false_pos.append(data)\n",
    "            else:\n",
    "                if (dataset[i]['target_scores']['No'] == 1):\n",
    "                    true_neg.append(data)\n",
    "                    correct += 1\n",
    "                else:\n",
    "                    false_neg.append(data)\n",
    "        else:\n",
    "            if results[i]['Match'][0:3] == 'yes':\n",
    "                if results[i]['Label'] == 1:\n",
    "                    true_pos.append(data)\n",
    "                    correct += 1\n",
    "                else:\n",
    "                    false_pos.append(data)\n",
    "            else:\n",
    "                if results[i]['Label'] == 0:\n",
    "                    true_neg.append(data)\n",
    "                    correct += 1\n",
    "                else:\n",
    "                    false_neg.append(data)\n",
    "    \n",
    "    name = dataset_names[d]\n",
    "    \n",
    "    true_pos_all[name], true_neg_all[name], false_pos_all[name], false_neg_all[name] = true_pos, true_neg, false_pos, false_neg\n",
    "    accuracy[name], precision[name], recall[name] = correct/len(data), len(true_pos) / (len(true_pos) + len(false_pos)), len(true_pos) / (len(true_pos) + len(false_neg))\n",
    "    f1[name] = 2 * len(true_pos) / (2 * len(true_pos) + len(false_pos) + len(false_neg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f'./{folder}/true_pos.json', 'w') as f:\n",
    "        json.dump(true_pos_all, f, indent=4)\n",
    "with open(f'./{folder}/true_neg.json', 'w') as f:\n",
    "        json.dump(true_neg_all, f, indent=4)\n",
    "with open(f'./{folder}/false_pos.json', 'w') as f:\n",
    "        json.dump(false_pos_all, f, indent=4)\n",
    "with open(f'./{folder}/false_neg.json', 'w') as f:\n",
    "        json.dump(false_neg_all, f, indent=4)\n",
    "# with open(f'./{folder}/accuracy.json', 'w') as f:\n",
    "#         json.dump(accuracy, f, indent=4)\n",
    "with open(f'./{folder}/f1.json', 'w') as f:\n",
    "        json.dump(f1, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame()\n",
    "df['Precision'] = pd.Series(precision)\n",
    "df['Recall'] = pd.Series(recall)\n",
    "df['F1'] = pd.Series(f1)\n",
    "df['Precision'] *= 100\n",
    "df['Recall'] *= 100\n",
    "df['F1'] *= 100\n",
    "df.round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.mean(df['F1'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyze with schema results, sampled (test) set only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "\n",
    "folder = 'with-schema-gpt4-turbo'\n",
    "dataset_names = {\n",
    "    'abt-buy-sampled-gs_domain-complex-force': 'abt-buy-gs',\n",
    "    'walmart-amazon-sampled-gs_domain-complex-force': 'walmart-amazon-gs',\n",
    "    'amazon-google-sampled-gs_domain-complex-force': 'amazon-google-gs',\n",
    "    'dblp-scholar-sampled-gs_domain-complex-force': 'dblp-scholar-gs',\n",
    "    # 'wdcproducts-80cc-seen-sampled-250-gs-2_domain-complex-force': 'wdcproducts-80cc-seen-gs',\n",
    "    'dblp-acm-sampled-gs_domain-complex-force': 'dblp-acm-gs'\n",
    "}\n",
    "\n",
    "# with open(f\"./sampled_data.json\", \"r\") as f:\n",
    "#     in_sample_result_raw = json.load(f)\n",
    "#     in_sample_result = {}\n",
    "#     for k in in_sample_result_raw.keys():\n",
    "#         in_sample_result[k] = [tuple(i) for i in in_sample_result_raw[k]]\n",
    "\n",
    "# with open(os.path.join(f, 'test.ndjson'), 'r') as fi:\n",
    "#     dataset = [json.loads(line)['pair_id'] for line in fi]\n",
    "\n",
    "true_pos_all, true_neg_all, false_pos_all, false_neg_all = {}, {}, {}, {}\n",
    "accuracy, precision, recall, f1 = {}, {}, {}, {}\n",
    "\n",
    "for d_k in dataset_names.keys():\n",
    "    name = dataset_names[d_k]\n",
    "    \n",
    "    with open(f'../libem-sample-data/{name[:-3]}/test.ndjson', 'r') as fi:\n",
    "        in_set = [json.loads(line)['pair_id'] for line in fi]\n",
    "    \n",
    "    # read in results\n",
    "    with open(f\"./{folder}/{name}_results.json\", \"r\") as f:\n",
    "        data = json.load(f)\n",
    "        \n",
    "        # process results\n",
    "        true_pos, true_neg, false_pos, false_neg = [], [], [], []\n",
    "        correct, pos, neg = 0, 0, 0\n",
    "    \n",
    "        for d in data:\n",
    "            # find if in sampled data\n",
    "            prod1 = d['product 1']\n",
    "            prod2 = d['product 2']\n",
    "            \n",
    "            # id_pair = (prod1[7: prod1.find('\",\"')], prod2[6: prod2.find('\",\"')])\n",
    "            # if not id_pair in in_sample_result[name]:\n",
    "            #     continue\n",
    "            id_pair = prod1[7: prod1.find('\",\"')] + '#' + prod2[6: prod2.find('\",\"')]\n",
    "            if not id_pair in in_set:\n",
    "                continue\n",
    "            \n",
    "            output = {'product 1': d['product 1'], 'product 2': d['product 2']}\n",
    "            if d['response'][0:3] == 'Yes':\n",
    "                if d['label'] == '1':\n",
    "                    true_pos.append(output)\n",
    "                    correct += 1\n",
    "                    pos += 1\n",
    "                else:\n",
    "                    false_pos.append(output)\n",
    "                    neg += 1\n",
    "            else:\n",
    "                if d['label'] == '0':\n",
    "                    true_neg.append(output)\n",
    "                    correct += 1\n",
    "                    neg += 1\n",
    "                else:\n",
    "                    false_neg.append(output)\n",
    "                    pos += 1\n",
    "        \n",
    "        # make sure filtered set has same length as test set\n",
    "        assert(len(in_set) == len(true_pos) + len(true_neg) + len(false_pos) + len(false_neg))\n",
    "        \n",
    "        true_pos_all[name], true_neg_all[name], false_pos_all[name], false_neg_all[name] = true_pos, true_neg, false_pos, false_neg\n",
    "        accuracy[name], precision[name], recall[name] = correct/(pos + neg), len(true_pos) / (len(true_pos) + len(false_pos)), len(true_pos) / (len(true_pos) + len(false_neg))\n",
    "        f1[name] =  2 * len(true_pos) / (2 * len(true_pos) + len(false_pos) + len(false_neg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f'./{folder}/true_pos.json', 'w') as f:\n",
    "        json.dump(true_pos_all, f, indent=4)\n",
    "with open(f'./{folder}/true_neg.json', 'w') as f:\n",
    "        json.dump(true_neg_all, f, indent=4)\n",
    "with open(f'./{folder}/false_pos.json', 'w') as f:\n",
    "        json.dump(false_pos_all, f, indent=4)\n",
    "with open(f'./{folder}/false_neg.json', 'w') as f:\n",
    "        json.dump(false_neg_all, f, indent=4)\n",
    "# with open(f'./{folder}/accuracy.json', 'w') as f:\n",
    "#         json.dump(accuracy, f, indent=4)\n",
    "with open(f'./{folder}/f1.json', 'w') as f:\n",
    "        json.dump(f1, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame()\n",
    "df['Precision'] = pd.Series(precision)\n",
    "df['Recall'] = pd.Series(recall)\n",
    "df['F1'] = pd.Series(f1)\n",
    "df['Precision'] *= 100\n",
    "df['Recall'] *= 100\n",
    "df['F1'] *= 100\n",
    "df.round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.mean(df['F1'])\n",
    "accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyze libem run results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "file = '../libem/benchmark/results/???.json'\n",
    "\n",
    "true_pos_all, true_neg_all, false_pos_all, false_neg_all = {}, {}, {}, {}\n",
    "accuracy, precision, recall, f1 = {}, {}, {}, {}\n",
    "\n",
    "# for d_k in dataset_names.keys():\n",
    "#     name = dataset_names[d_k]\n",
    "name = '???'\n",
    "    \n",
    "# read in results\n",
    "with open(file, \"r\") as f:\n",
    "    data = json.load(f)\n",
    "    \n",
    "    # process results\n",
    "    true_pos, true_neg, false_pos, false_neg = [], [], [], []\n",
    "    correct, pos, neg = 0, 0, 0\n",
    "\n",
    "    for d in data:\n",
    "        # find if in sampled data\n",
    "        prod1 = d['entity_1']\n",
    "        prod2 = d['entity_2']\n",
    "        \n",
    "        output = {'product 1': prod1, 'product 2': prod2}\n",
    "        if d['pred'][0:3] == 'yes':\n",
    "            if d['label'] == 1:\n",
    "                true_pos.append(output)\n",
    "                correct += 1\n",
    "                pos += 1\n",
    "            else:\n",
    "                false_pos.append(output)\n",
    "                neg += 1\n",
    "        else:\n",
    "            if d['label'] == 0:\n",
    "                true_neg.append(output)\n",
    "                correct += 1\n",
    "                neg += 1\n",
    "            else:\n",
    "                false_neg.append(output)\n",
    "                pos += 1\n",
    "    \n",
    "    true_pos_all[name], true_neg_all[name], false_pos_all[name], false_neg_all[name] = true_pos, true_neg, false_pos, false_neg\n",
    "    accuracy[name], precision[name], recall[name] = correct/len(data), len(true_pos) / (len(true_pos) + len(false_pos)), len(true_pos) / (len(true_pos) + len(false_neg))\n",
    "    f1[name] =  2 * len(true_pos) / (2 * len(true_pos) + len(false_pos) + len(false_neg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame()\n",
    "df['Precision'] = pd.Series(precision)\n",
    "df['Recall'] = pd.Series(recall)\n",
    "df['F1'] = pd.Series(f1)\n",
    "df['Precision'] *= 100\n",
    "df['Recall'] *= 100\n",
    "df['F1'] *= 100\n",
    "df.round(2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spacegpt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
